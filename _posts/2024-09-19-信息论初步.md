---
layout: post #默认post布局

title: 信息论初步
description: 信息论的基本概念
author: [diexie, ] # 此处可以简写，详细添加在 _data/authors.yml 中
date: 2024-09-19 12:20:00 +0800

pin: false # 置顶
categories: [学科,信息] # 分类,第一个为主分类
tags: [学科] # 标签

math: true # 支持数学公式
toc: true # 支持侧边目录
comments: true # 支持评论
mermaid: true # 支持mermaid图表

# cdn: https://xxxx.com/ # CDN加速，可以与media_subpath配合使用：[site.cdn/][page.media_subpath/]file.ext
# media_subpath: /assets/img/ # 媒体文件路径，用于简写本地图片等媒体文件路径，注意：封面图路径**会受影响**

image:
  path: https://s2.loli.net/2024/09/19/C7cxZDs961FOJLU.png # 封面图
  lqip: data:image/webp # lqip占位符，用于图片懒加载
  alt: 『Fifth Avenue, New York』George Luks 1920 # 头图描述
---
## 信息论初步

1. 自信息量(香农信息量): 一个随机事件发生某一结果后所带来的信息量称为自信息量，简称自信息。定义为其发生概率对数的负值

  $$
    I(x) = \log \frac{1}{P(x)} = -\log P(x) 
  $$

  log以2为底时，单位为比特(bit)

2. 信息熵: 信息熵是对所有可能发生的事件的信息量的期望值，也就是随机变量的**不确定性**的度量

  $$
    H(X) = -\sum_{i=1}^{n}P(x_i)\log P(x_i)
  $$

3. 最大离散熵定理(香农不等式)：对于离散随机变量，当其可能的取值等概分布时，其熵达到最大值。即信源X中包含N个不同离散消息时，信源熵H(X)有

    $$
      H(X) \leq \log N
    $$

    广义香农不等式：对于任意一个信源X，其熵H(X)不大于其最大离散熵

    $$
    \log_2n=H_0\geq H_1\geq H_2\geq\cdots\geq H_m>H_\infty
    $$

4. 联合自信息量: 两个事件同时发生的信息量

  $$
    I(x,y) = \log \frac{1}{P(x,y)} = -\log P(x,y)
  $$

5. 条件自信息量: 已知事件Y发生的情况下，事件X发生的信息量

  $$
    I(X|Y) = \log \frac{1}{P(X|Y)} = -\log P(X|Y)
  $$

6. 条件熵: 已知随机变量Y的条件下，随机变量X的条件熵

  $$
    H(X|Y) = -\sum_{i=1}^{n}\sum_{j=1}^{m}P(x_i,y_j)\log P(x_i|y_j)
  $$

5. 联合信息熵: 两个随机变量的联合信息熵

  $$
    H(X,Y) = -\sum_{i=1}^{n}\sum_{j=1}^{m}P(x_i,y_j)\log P(x_i,y_j)
  $$

7. 联合熵与条件熵的关系: 

  $$
    H(X,Y) = H(X|Y) + H(Y)
  $$

8. 互信息量: 两个随机变量之间的互信息量，表示一个随机变量中包含另一个随机变量的信息量

  $$
    I(X;Y) = \sum_{i=1}^{n}\sum_{j=1}^{m}P(x_i,y_j)\log \frac{P(x_i,y_j)}{P(x_i)P(y_j)}
  $$

9. 冗余度: 表示一个信源在实际发出消息时所包含的多余消息

    $$
      I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
    $$
    信源效率: $\eta=\frac{H_\infty}{H_0}$

    信源冗余度: $\rho=1-\eta$
