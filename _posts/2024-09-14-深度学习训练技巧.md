---
layout: post #默认post布局

title: 深度学习训练技巧
description: 这个笔记主要记录一些ml的超参数调节及tricks

author: [diexie, ] # 此处可以简写，详细添加在 _data/authors.yml 中
date: 2024-09-14 18:00:00 +0800

pin: false # 置顶
categories: [深度学习] # 分类,第一个为主分类
tags: [深度学习,Tricks,超参数] # 标签

math: true # 支持数学公式
toc: true # 支持侧边目录
comments: false # 支持评论
mermaid: true # 支持mermaid图表

# cdn: https://xxxx.com/ # CDN加速，可以与media_subpath配合使用：[site.cdn/][page.media_subpath/]file.ext
# media_subpath: /assets/img/ # 媒体文件路径，用于简写本地图片等媒体文件路径，注意：封面图路径**会受影响**

image:
  path: https://s2.loli.net/2024/09/14/WsX9e7Aro3YQJZG.png # 封面图
  lqip: data:image/webp # lqip占位符，用于图片懒加载
  alt: 『Wood Gatherers』Anton Mauve 年代未知 # 头图描述
---

为以下系列教程的笔记：

{% include embed/youtube.html id='QW6uINn7uGk' %}

## 1. tricks: learning rate/optimizer/batch/one-hot encoding/batch_normalization

### 1.1局部最小值 (local minima) 鞍点 (saddle point)

* 主要叙述出现有时无法update的情况
* local minimal 与 saddle point 是loss function 微分为0点处, saddle point是可以逃离的

   ![image.png](https://s2.loli.net/2024/09/14/1QLk9qrag5lSFwj.png)

### 1.2 batch批训练与momentum动量

* 使用大batch与小batch的优劣
  1. **单次运算时间**: 由于GPU计算时为并行, 所以在batch不是过分大的情况下, 使用大batch与小batch运算一次loss的时间相近
  2. **跑完整个epoch所需的时间**: 由于大batch在相同epoch下**反向传播次数**小于小batch,所以大batch运行一次epoch时间小于小batch
  3. **训练效果:** 小batch训练**噪声(noise)**较大,update方向随机性较大, 反而可以提高运算结果; 而大batch可能无法获得很好地结果
  4. Testing效果: 小batch也对testing有正面效果

![image.png](https://s2.loli.net/2024/09/14/J95glnwDyYiSE6r.png)

* momentum冲量: 即在训练计算gradient后再加上一项之前的gradient, 提供"惯性", 有助于逃离local minimal

### 1.3 optimizer/lr

* loss不再减小的另一种可能: learning rate 太大, 使得优化参数在山谷两侧来回摆动!

   ![image.png](https://s2.loli.net/2024/09/14/c7UnRGw9sYhTy8N.png)

* 平衡训练速度与训练效果的方法: 自适应learning rate

  1. 常规gradient descent方法:
     $$
   \begin{gathered}\boldsymbol{\theta}_i^{\boldsymbol{t}+1}\leftarrow\boldsymbol{\theta}_i^\boldsymbol{t}-\eta\boldsymbol{g}_i^\boldsymbol{t}\\\boldsymbol{g}_i^t=\frac{\partial L}{\partial\boldsymbol{\theta}_i}|_{\boldsymbol{\theta}=\boldsymbol{\theta}^t}\end{gathered}
     $$

     其中

     $$
     \eta 为learning rate\\
     g为gradient
     $$
     
   2. 使用自适应learning rate 的一般表达:
      $$
      \theta_i^{t+1}\leftarrow\theta_i^t-\frac{\eta}{\sigma_i^t}\boldsymbol{g}_i^t\\
      其中\sigma为新引入的调节参数
      $$
  
* **自适应learning rate**
  
   1. 法1: Root Square Mean方法(Adagrad)
   
      * 即将调节参数σ设置为之前所有gradient的root square mean; 使得在平坦的地方learning rate变大, 加快收敛
   
      $$
      \theta_i^{t+1}\leftarrow\theta_i^t-\frac{\eta}{\sigma_i^t}\boldsymbol{g}_i^t\\
      \begin{aligned}\theta_t^1&\leftarrow\theta_t^0-\frac\eta{\sigma_t^0}g_t^0&\quad\sigma_t^0&=\sqrt{\left(g_t^0\right)^2}=\left|g_t^0\right|\\\theta_t^2&\leftarrow\theta_t^1-\frac\eta{\sigma_t^1}g_t^1&\quad\sigma_t^1&=\sqrt{\frac12\left[\left(g_t^0\right)^2+\left(g_t^1\right)^2\right]}\\\theta_t^3&\leftarrow\theta_t^2-\frac\eta{\sigma_t^2}g_t^2&\quad\sigma_t^2&=\sqrt{\frac13\left[\left(g_t^0\right)^2+\left(g_t^1\right)^2+\left(g_t^2\right)^2\right]}\\&\\\\theta_t^{t+1}&\leftarrow\theta_t^t-\frac\eta{\sigma_t^t}g_t^t&\quad\sigma_t^t&=\sqrt{\frac1{t+1}\sum_{i=0}^t\left(g_t^t\right)^2}\end{aligned}
      $$
   
   
   
   2. 法2: RMSProp
   
      * 是adagrad的改进, 引入一个可调节的权重α来决定当前gradient的重要性
   
      $$
      \theta_i^{t+1}\leftarrow\theta_i^t-\frac{\eta}{\sigma_i^t}\boldsymbol{g}_i^t\\
      \begin{aligned}\theta_t^1&\leftarrow\theta_t^0-\frac\eta{\sigma_t^0}g_t^0&\quad\sigma_t^0&=\sqrt{\left(g_t^0\right)^2}\\\theta_t^2&\leftarrow\theta_t^1-\frac\eta{\sigma_t^1}g_t^1&\quad\sigma_t^1&=\sqrt{a\left(\sigma_t^0\right)^2+(1-\alpha)\left(g_t^1\right)^2}\\\theta_t^3&\leftarrow\theta_t^2-\frac\eta{\sigma_t^2}g_t^2&\quad\sigma_t^2&=\sqrt{\alpha\left(\sigma_t^1\right)^2+(1-\alpha)\left(g_t^2\right)^2}\\&\quad\vdots\\\theta_t^{t+1}&\leftarrow\theta_t^t-\frac\eta{\sigma_t^t}g_t^t&\quad\sigma_t^t&=\sqrt{a\left(\sigma_t^{t-1}\right)^2+(1-\alpha)\left(g_t^t\right)^2}\end{aligned}
      $$
      * 注: Adam是 RmSprop + momentun 
   
* 自适应learning rate 潜在的问题


​	当某一个方向的梯度过小时, 该方向的可调参数σ会进行累计, 在某一时期变得很大,如图

   ![image.png](https://s2.loli.net/2024/09/14/XjMqk93hiZt741l.png)

* learning rate schedule

   $$
   \theta_i^{t+1}\leftarrow\theta_i^t-\frac{\eta}{\sigma_i^t}\boldsymbol{g}_i^t\\
   $$
   通过调节learning rate η, 来规划learning rate

   * 两种规划方法:

   ![image.png](https://s2.loli.net/2024/09/14/W2ZblDf3mSLMdJG.png)

   1. lr decay
      1. 显然, 逐步减小lr
   2. warm up
      1. 先增大后减小rl
      2. 理解: 由于一开始训练时统计是不精准的,存在大方差, 置信度较低, 所以我们不让参数走太远

### 1.4 one-hot encoding与loss function

> 使用one-hot编码可以使在做分类问题时, 两类之间的"距离"是一样的, 级不同类别之间的相关性是相同的

* loss function

  1. MSE
     $$
     e=\sum_{i}(\widehat{y}_{i}-y_{i}^{\prime})^{2}
     $$

  2. Cross-entropy
     $$
     e=-\sum_i\widehat{y}_iln\boldsymbol{y}_i^{\prime}
     $$

     > minimize cross entropy 就是在 maxmize likelihood
     >
     > cross-entropy 优于 MSE(在分类问题中)

### 1.5 Batch Normalization批处理标准化(前处理)

* 适用于batch较大的情况下, 可以使得梯度下降更快("把山削平")

* 在做testing时, 我们不能等到获得一个batch再做输出, 因此batch normalization不合适; 解决方法: **moving normalization**
  $$
  \tilde{z}=\frac{z-\mu}{\sigma}\\
  \begin{aligned}\mu^{1}&\quad\mu^{2}&\quad\mu^{3}&\quad....&\quad\mu^{t}\\&\quad\overline{\mu}&\leftarrow p\overline{\boldsymbol{\mu}}+(1-p)\boldsymbol{\mu}^{t}\end{aligned}
  $$

* 更多normalization: 

  - Batch Renormalization
  - https://arxiv.org/abs/1702.03275
  - Layer Normalization
  - https://arxiv.org/abs/1607.06450
  - Instance Normalization
  - https://arxiv.org/abs/1607.08022
  - Group Normalization
  - https://arxiv.org/abs/1803.08494
  - Weight Normalization
  - https://arxiv.org/abs/1602.07868
  - Spectrum Normalization
  - https://arxiv.org/abs/1705.10941  
